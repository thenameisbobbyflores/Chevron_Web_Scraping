{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997650c4",
   "metadata": {},
   "source": [
    "# Install and Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199e3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585b8a0",
   "metadata": {},
   "source": [
    "# Mapping for States / Provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a636d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Mapping for ease of use\n",
    "state_mapping = {\n",
    "    'AL': 'Alabama',\n",
    "    'AK': 'Alaska',\n",
    "    'AZ': 'Arizona',\n",
    "    'AR': 'Arkansas',\n",
    "    'CA': 'California',\n",
    "    'CO': 'Colorado',\n",
    "    'CT': 'Connecticut',\n",
    "    'DE': 'Delaware',\n",
    "    'FL': 'Florida',\n",
    "    'GA': 'Georgia',\n",
    "    'HI': 'Hawaii',\n",
    "    'ID': 'Idaho',\n",
    "    'IL': 'Illinois',\n",
    "    'IN': 'Indiana',\n",
    "    'IA': 'Iowa',\n",
    "    'KS': 'Kansas',\n",
    "    'KY': 'Kentucky',\n",
    "    'LA': 'Louisiana',\n",
    "    'ME': 'Maine',\n",
    "    'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts',\n",
    "    'MI': 'Michigan',\n",
    "    'MN': 'Minnesota',\n",
    "    'MS': 'Mississippi',\n",
    "    'MO': 'Missouri',\n",
    "    'MT': 'Montana',\n",
    "    'NE': 'Nebraska',\n",
    "    'NV': 'Nevada',\n",
    "    'NH': 'New Hampshire',\n",
    "    'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico',\n",
    "    'NY': 'New York',\n",
    "    'NC': 'North Carolina',\n",
    "    'ND': 'North Dakota',\n",
    "    'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'RI': 'Rhode Island',\n",
    "    'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'TX': 'Texas',\n",
    "    'UT': 'Utah',\n",
    "    'VT': 'Vermont',\n",
    "    'VA': 'Virginia',\n",
    "    'WA': 'Washington',\n",
    "    'WV': 'West Virginia',\n",
    "    'WI': 'Wisconsin',\n",
    "    'WY': 'Wyoming',\n",
    "    'DC': 'Washington D.C.',\n",
    "    'AB': 'Alberta', \n",
    "    'BC': 'British Columbia', \n",
    "    'MB': 'Manitoba', \n",
    "    'NB': 'New Brunswick', \n",
    "    'NL': 'Newfoundland and Labrador', \n",
    "    'NS': 'Nova Scotia', \n",
    "    'NT': 'Northwest Territories', \n",
    "    'NU': 'Nunavut', \n",
    "    'ON': 'Ontario', \n",
    "    'PE': 'Prince Edward Island', \n",
    "    'QC': 'Quebec', \n",
    "    'SK': 'Saskatchewan', \n",
    "    'YT': 'Yukon'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee372e",
   "metadata": {},
   "source": [
    "# Initialize the Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584da7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_webpage(base_url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(base_url)    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc822d",
   "metadata": {},
   "source": [
    "# Accept Cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45346ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept_cookies(driver, wait):\n",
    "    try:\n",
    "        cookie_accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Accept')] | //button[contains(text(), 'OK')]\")))\n",
    "        cookie_accept_button.click()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while accepting cookies: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494a252",
   "metadata": {},
   "source": [
    "# GDPR Alert Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb3a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdpr_alert_check(driver, wait):\n",
    "    try:\n",
    "        gdpr_alert = driver.find_element(By.ID, \"gdpr-alert\")\n",
    "        if gdpr_alert.is_displayed():\n",
    "            gdpr_button = gdpr_alert.find_element(By.TAG_NAME, \"button\")\n",
    "            gdpr_button.click()\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"search-results-list\")))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking GDPR alert: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a613a",
   "metadata": {},
   "source": [
    "# Prep the webpage for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c3f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_items(driver, fixed_wait_time):    \n",
    "    wait = WebDriverWait(driver, fixed_wait_time)\n",
    "    accept_cookies(driver, wait)\n",
    "    gdpr_alert_check(driver,wait)    \n",
    "    job_list = []\n",
    "    return driver, wait, job_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092801cc",
   "metadata": {},
   "source": [
    "# Wait for correct page to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3637c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_page(driver, page_number):\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.text_to_be_present_in_element((By.ID, \"search-results\"), f\"data-current-page={page_number}\")\n",
    "        )\n",
    "        print(f\"Page {page_number} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while waiting for page {page_number}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9912ba",
   "metadata": {},
   "source": [
    "# Get page contents and update CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1272c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_contents_and_update_csv(driver, job_list, wait, page_number):\n",
    "    try:\n",
    "        # Wait for the correct page to load\n",
    "        wait_for_page(driver, page_number)\n",
    "\n",
    "        # Extract the data-current-page attribute from the search-results section\n",
    "        current_page = driver.find_element(By.ID, \"search-results\").get_attribute(\"data-current-page\")\n",
    "\n",
    "        # Find the job list element by its ID\n",
    "        job_list_element = driver.find_element(By.ID, \"search-results-list\")\n",
    "\n",
    "        # Use BeautifulSoup to parse the HTML content of the job list element\n",
    "        soup = BeautifulSoup(job_list_element.get_attribute(\"innerHTML\"), 'lxml')\n",
    "        \n",
    "        # Find all list items within the parsed job list element\n",
    "        job_items = soup.find_all('li')  # Define job_items here\n",
    "\n",
    "        for job_item in job_items:\n",
    "            job_name = job_item.find('h2').text.strip()\n",
    "            job_location = job_item.find('span', {'class': 'job-location'}).text.strip()\n",
    "            job_link = job_item.find('a')['href']\n",
    "\n",
    "            city, state_abbr = job_location.split(', ')\n",
    "            state = state_mapping.get(state_abbr, state_abbr)\n",
    "\n",
    "            url = f\"https://careers.chevron.com{job_link}\"\n",
    "            job_list.append([job_name, url, city, state, current_page, page_number])\n",
    "\n",
    "        # Save the job information to a CSV file with pipe character as delimiter\n",
    "        with open('job_information.csv', 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile, delimiter='|')  # Use '|' as the delimiter\n",
    "\n",
    "            # Write header row\n",
    "            csv_writer.writerow(['Job Name', 'URL', 'City', 'State', 'Current Page', 'Page Number'])\n",
    "\n",
    "            # Write job information rows\n",
    "            csv_writer.writerows(job_list)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping page {page_number}: {str(e)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5338590",
   "metadata": {},
   "source": [
    "# Determine if there is a next page and click if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c7671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_and_click_next_page(driver, wait):\n",
    "    try:\n",
    "        next_page_link = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='next']\")))\n",
    "        if 'disabled' in next_page_link.get_attribute(\"class\"):\n",
    "            return False\n",
    "        \n",
    "        # Scroll the next_page_link into view\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_to_element(next_page_link).perform()\n",
    "        \n",
    "        # Click the \"Next Page\" link\n",
    "        next_page_link.click()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while determining and clicking the next page: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3f7d2",
   "metadata": {},
   "source": [
    "# Run the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b97f321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while accepting cookies: Message: element click intercepted: Element is not clickable at point (878, 1252)\n",
      "  (Session info: chrome=117.0.5938.132)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6925D7892+54818]\n",
      "\t(No symbol) [0x00007FF692546AC2]\n",
      "\t(No symbol) [0x00007FF6923FDA3B]\n",
      "\t(No symbol) [0x00007FF6924447CB]\n",
      "\t(No symbol) [0x00007FF692442B99]\n",
      "\t(No symbol) [0x00007FF692440968]\n",
      "\t(No symbol) [0x00007FF69243FA23]\n",
      "\t(No symbol) [0x00007FF69243571F]\n",
      "\t(No symbol) [0x00007FF69245EAAA]\n",
      "\t(No symbol) [0x00007FF692435036]\n",
      "\t(No symbol) [0x00007FF69245ECC0]\n",
      "\t(No symbol) [0x00007FF6924775A2]\n",
      "\t(No symbol) [0x00007FF69245E883]\n",
      "\t(No symbol) [0x00007FF692433691]\n",
      "\t(No symbol) [0x00007FF6924348D4]\n",
      "\tGetHandleVerifier [0x00007FF69293B992+3610402]\n",
      "\tGetHandleVerifier [0x00007FF692991860+3962352]\n",
      "\tGetHandleVerifier [0x00007FF692989D4F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF692673646+693206]\n",
      "\t(No symbol) [0x00007FF692551628]\n",
      "\t(No symbol) [0x00007FF69254D934]\n",
      "\t(No symbol) [0x00007FF69254DA62]\n",
      "\t(No symbol) [0x00007FF69253E113]\n",
      "\tBaseThreadInitThunk [0x00007FFA3C91257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFA3DC4AA68+40]\n",
      "\n",
      "An error occurred while waiting for page 1: Message: \n",
      "\n",
      "An error occurred while waiting for page 2: Message: \n",
      "\n",
      "An error occurred while waiting for page 3: Message: \n",
      "\n",
      "An error occurred while waiting for page 4: Message: \n",
      "\n",
      "An error occurred while waiting for page 5: Message: \n",
      "\n",
      "An error occurred while determining and clicking the next page: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6925D7892+54818]\n",
      "\t(No symbol) [0x00007FF692546AC2]\n",
      "\t(No symbol) [0x00007FF6923FDA3B]\n",
      "\t(No symbol) [0x00007FF69243E4FC]\n",
      "\t(No symbol) [0x00007FF69243E67C]\n",
      "\t(No symbol) [0x00007FF692479627]\n",
      "\t(No symbol) [0x00007FF69245EAEF]\n",
      "\t(No symbol) [0x00007FF6924775A2]\n",
      "\t(No symbol) [0x00007FF69245E883]\n",
      "\t(No symbol) [0x00007FF692433691]\n",
      "\t(No symbol) [0x00007FF6924348D4]\n",
      "\tGetHandleVerifier [0x00007FF69293B992+3610402]\n",
      "\tGetHandleVerifier [0x00007FF692991860+3962352]\n",
      "\tGetHandleVerifier [0x00007FF692989D4F+3930847]\n",
      "\tGetHandleVerifier [0x00007FF692673646+693206]\n",
      "\t(No symbol) [0x00007FF692551628]\n",
      "\t(No symbol) [0x00007FF69254D934]\n",
      "\t(No symbol) [0x00007FF69254DA62]\n",
      "\t(No symbol) [0x00007FF69253E113]\n",
      "\tBaseThreadInitThunk [0x00007FFA3C91257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFA3DC4AA68+40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Provide the base_url\n",
    "    base_url = \"https://careers.chevron.com/search-jobs/United%20States/35016/2/6252001/39x76/-98x5/50/2\"\n",
    "    \n",
    "    # Obtain the html text from the website\n",
    "    html_text = requests.get(base_url).text\n",
    "\n",
    "    # Store the information as soup\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    \n",
    "    fixed_wait_time = 10\n",
    "    driver, wait, job_list = load_items(initialize_webpage(base_url), fixed_wait_time)  # Load items and get wait object\n",
    "    page_number = 1\n",
    "    total_pages = int(soup.find('section', {'id': 'search-results'})['data-total-pages'])  # Extract total pages\n",
    "\n",
    "    # Loop to scrape data from pages\n",
    "    for page_number in range(1, total_pages + 1):\n",
    "        get_page_contents_and_update_csv(driver, job_list, wait, page_number)\n",
    "        determine_and_click_next_page(driver, wait)\n",
    "\n",
    "    # Close the webpage\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328689ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
